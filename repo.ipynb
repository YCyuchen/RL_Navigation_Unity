{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import torch\n",
    "from collections import deque\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from dqn_agent import Agent\n",
    "from model import QNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question for banana navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several questions  \n",
    "Q1: why there are two Q value in this project Q_expected, Q_targets and why there are two model qnetwork_target, qnetwork_local?\n",
    " - Q_targets: say we take a single move in current state and see what reward(Qvalue) we can get, thus, reward+Q(s',a') become the output(target) that we want Q(s,a) to be, and this is the defination of qnetwork_target.  \n",
    "  `Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))`  # done=0 or 1\n",
    "  \n",
    " - Q_expected: Q value from the model:  \n",
    "  `Q_expected = self.qnetwork_local(states).gather(1, actions)`\n",
    "  \n",
    " - qnetwork_local:  \n",
    "     the weights are calculated by the current(local) batch\n",
    "\n",
    "  \n",
    " - qnetwork_target:  \n",
    "     designed to be a global model that can give a ideal Q value in each state-action pair regardless of the variation of   states. \n",
    " \n",
    "Explaination: why we need to network qnetwork_local and qnetwork_target?  \n",
    "  > At every step of training, the Q-network’s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and (local batch)estimated Q-values.  \n",
    "   **We want the qnetwork_target can handle every kinds of state and give ideal reward like the qnetwork_local dose??** so we use the mes_loss(mean square loss) to calculate the mean square difference between  Q_expected and Q_targets.  \n",
    "   the weights in qnetwork_target are then slowly update by the function: `θ_target = τ*θ_local + (1-τ)*θ_target`\n",
    "\n",
    "----\n",
    "Q2:can i regard the Q_value: **Q(St,At)** as the reward value?   \n",
    "Reply: No, Q_value is a quantitative measure of an action in a state.  \n",
    "**Me: Maybe my explaination of loss function (see above) are not correct. As i don't understand why reward valuse is only used in Q_targets, not in Q_expected. How can the weights in Q_expected being updated without knowing the supervised info(reward value)?**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning Network for banana navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In this project, the goal of the agent is to collect yellow banana and avoid blue banana.***\n",
    "\n",
    "Given a state as a vector in length=37(containing the agent's velocity, along with ray-based perception of objects around agent's forward direction), the agent are expected to select the action that bring biggest **current** value.  \n",
    "Although the state is discrete and not very large(37), sampling the states will influence the performance. Thus, DQN is sellected as the method for this project.\n",
    "\n",
    "**how dose DQN work?**  \n",
    "- feed forward  \n",
    "> taking a state value as input, the feed-forward QNetwork(in this project, a three layers' fully connected network) output the values on all actions. the action is selected with epsilon-greedy method. With probability $\\epsilon$ the agent randomly selected an action, and with prob (1- $\\epsilon$), the agent select the action with maxium output value. \n",
    "\n",
    "- back propagation\n",
    "> loss is calculated by the mean square difference of Q_expected, Q_targets. Q_expected is the network's value of the selected action while Q_targets is the reward of current action plus the Q_targets_next with discount rate $\\gamma$. I haven't totally understand why we doing this..., so i left some place here.\n",
    "\n",
    "**selection of the hyprer parameters**\n",
    " - $\\epsilon$(epsilon greedy action discount factor):  \n",
    "With prob=$\\epsilon$,the agent randomly selects an action.With prob=(1-$\\epsilon$), the agent selects the action with max $\\epsilon$ is decreaing with the increase of epoch, which means the agent tend to be more confinent of the action with the maxium action value (Q value)  \n",
    "$\\epsilon$ = $\\epsilon$ * 0.99 (0.99 is the multiplicative factor for decreasing the epsilon value)\n",
    " - $\\gamma$(discount factor, the influence of next state's Q value)  \n",
    " used in Q_targets, `Q_targets = rewards + (gamma * Q_targets_next * (1 - dones)) `\n",
    " - LR: learning rate for updating the weights in model\n",
    " - n_episodes: how many rounds are designed for agent to play the game\n",
    " - Adam optimzer: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the layer of the model and the curve while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]           4,864\n",
      "            Linear-2                   [-1, 64]           8,256\n",
      "            Linear-3                    [-1, 4]             260\n",
      "================================================================\n",
      "Total params: 13,380\n",
      "Trainable params: 13,380\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.05\n",
      "Estimated Total Size (MB): 0.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "state_size = 37\n",
    "action_size = 4\n",
    "seed = 0\n",
    "model = QNetwork(state_size, action_size, seed)\n",
    "summary(model, input_size=(37,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## here we plot the episode-reward curve durning training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Aaron Swartz](https://raw.githubusercontent.com/YCyuchen/RL_Navigation_Unity/master/score-episode-curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further work\n",
    "in this work, the agent achieve average 14 yellow bananas in each episode. This number can be expected to be higher.  \n",
    "For achieving this goal, severl methods are proposed here:\n",
    "- replace the model(3 fully connected layers) with 1d conv layer. This may help the agent to learn the spatial info from the environment and make better decision.\n",
    "- Improve the structure of DQN, like using Dueling DQN 、 Double Q-Learning、 or other [great off-policy RL algorithm](Demystifying Rainbow Baseline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
